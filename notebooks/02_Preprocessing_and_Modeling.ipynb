{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59452060",
   "metadata": {},
   "source": [
    "# Hospital Readmission Analytics - Part 2: Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d870a",
   "metadata": {},
   "source": [
    "This notebook covers the data preprocessing, feature engineering, and predictive modeling stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hospital Readmission Analytics - Data Preprocessing and Predictive Modeling\n",
    "Author: Data Analytics Portfolio Project\n",
    "Date: February 2026\n",
    "\n",
    "This script performs data preprocessing, feature engineering, and builds\n",
    "predictive models for hospital readmission risk.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             roc_auc_score, roc_curve, accuracy_score,\n",
    "                             precision_score, recall_score, f1_score)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# File paths\n",
    "DATA_PATH = '../data/diabetic_data.csv'\n",
    "OUTPUT_PATH = '../visualizations/'\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load data and perform initial preprocessing\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"LOADING AND PREPARING DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"\\nOriginal dataset: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    \n",
    "    # Replace '?' with NaN\n",
    "    df = df.replace('?', np.nan)\n",
    "    \n",
    "    # Create binary target: readmitted within 30 days (1) vs not readmitted (0)\n",
    "    df['readmitted_binary'] = (df['readmitted'] == '<30').astype(int)\n",
    "    \n",
    "    print(f\"\\nTarget variable created:\")\n",
    "    print(f\"  • Readmitted within 30 days: {df['readmitted_binary'].sum():,} ({df['readmitted_binary'].mean()*100:.2f}%)\")\n",
    "    print(f\"  • Not readmitted within 30 days: {(df['readmitted_binary']==0).sum():,} ({(1-df['readmitted_binary'].mean())*100:.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Clean and preprocess the dataset\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DATA CLEANING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Drop columns with too many missing values\n",
    "    high_missing_cols = ['weight', 'payer_code', 'medical_specialty']\n",
    "    df = df.drop(columns=high_missing_cols)\n",
    "    print(f\"\\nDropped {len(high_missing_cols)} columns with >40% missing values\")\n",
    "    \n",
    "    # Drop columns that are not useful for prediction\n",
    "    id_cols = ['encounter_id', 'patient_nbr']\n",
    "    df = df.drop(columns=id_cols)\n",
    "    print(f\"Dropped {len(id_cols)} ID columns\")\n",
    "    \n",
    "    # Handle medication columns (many have only 'No' values)\n",
    "    medication_cols = [col for col in df.columns if col in [\n",
    "        'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n",
    "        'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone',\n",
    "        'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide',\n",
    "        'examide', 'citoglipton', 'insulin', 'glyburide-metformin',\n",
    "        'glipizide-metformin', 'glimepiride-pioglitazone', \n",
    "        'metformin-rosiglitazone', 'metformin-pioglitazone'\n",
    "    ]]\n",
    "    \n",
    "    # Keep only medications with meaningful variation\n",
    "    meds_to_keep = []\n",
    "    for col in medication_cols:\n",
    "        if df[col].value_counts().get('No', 0) / len(df) < 0.95:\n",
    "            meds_to_keep.append(col)\n",
    "    \n",
    "    meds_to_drop = [col for col in medication_cols if col not in meds_to_keep]\n",
    "    df = df.drop(columns=meds_to_drop)\n",
    "    print(f\"Dropped {len(meds_to_drop)} medication columns with <5% variation\")\n",
    "    print(f\"Kept {len(meds_to_keep)} medication columns: {meds_to_keep}\")\n",
    "    \n",
    "    # Drop original readmitted column (we have binary version)\n",
    "    df = df.drop(columns=['readmitted'])\n",
    "    \n",
    "    print(f\"\\nCleaned dataset: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create new features from existing ones\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FEATURE ENGINEERING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Total number of medications\n",
    "    df['total_medications'] = df['num_medications']\n",
    "    \n",
    "    # Total visits (outpatient + emergency + inpatient)\n",
    "    df['total_visits'] = (df['number_outpatient'] + \n",
    "                          df['number_emergency'] + \n",
    "                          df['number_inpatient'])\n",
    "    \n",
    "    # Has emergency visits\n",
    "    df['has_emergency'] = (df['number_emergency'] > 0).astype(int)\n",
    "    \n",
    "    # Has prior inpatient visits\n",
    "    df['has_prior_inpatient'] = (df['number_inpatient'] > 0).astype(int)\n",
    "    \n",
    "    # Age group numeric (extract middle value)\n",
    "    age_mapping = {\n",
    "        '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,\n",
    "        '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,\n",
    "        '[80-90)': 85, '[90-100)': 95\n",
    "    }\n",
    "    df['age_numeric'] = df['age'].map(age_mapping)\n",
    "    \n",
    "    # Medication change indicator\n",
    "    df['med_changed'] = (df['change'] == 'Ch').astype(int)\n",
    "    \n",
    "    # On diabetes medication\n",
    "    df['on_diabetesMed'] = (df['diabetesMed'] == 'Yes').astype(int)\n",
    "    \n",
    "    print(f\"\\nCreated {6} new features:\")\n",
    "    print(\"  • total_medications\")\n",
    "    print(\"  • total_visits\")\n",
    "    print(\"  • has_emergency\")\n",
    "    print(\"  • has_prior_inpatient\")\n",
    "    print(\"  • age_numeric\")\n",
    "    print(\"  • med_changed\")\n",
    "    print(\"  • on_diabetesMed\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_for_modeling(df):\n",
    "    \"\"\"Prepare data for machine learning\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PREPARING DATA FOR MODELING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Select features for modeling\n",
    "    numeric_features = [\n",
    "        'time_in_hospital', 'num_lab_procedures', 'num_procedures',\n",
    "        'num_medications', 'number_outpatient', 'number_emergency',\n",
    "        'number_inpatient', 'number_diagnoses', 'age_numeric',\n",
    "        'total_visits', 'has_emergency', 'has_prior_inpatient',\n",
    "        'med_changed', 'on_diabetesMed'\n",
    "    ]\n",
    "    \n",
    "    categorical_features = ['race', 'gender', 'admission_type_id', \n",
    "                           'discharge_disposition_id', 'admission_source_id']\n",
    "    \n",
    "    # Handle missing values in categorical features\n",
    "    for col in categorical_features:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        df[col + '_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # Select final features\n",
    "    feature_cols = numeric_features + [col + '_encoded' for col in categorical_features]\n",
    "    \n",
    "    # Remove rows with missing values in feature columns\n",
    "    df_model = df[feature_cols + ['readmitted_binary']].dropna()\n",
    "    \n",
    "    print(f\"\\nFinal dataset for modeling: {df_model.shape[0]:,} rows × {len(feature_cols)} features\")\n",
    "    print(f\"Features used: {len(feature_cols)}\")\n",
    "    print(f\"  • Numeric features: {len(numeric_features)}\")\n",
    "    print(f\"  • Categorical features (encoded): {len(categorical_features)}\")\n",
    "    \n",
    "    # Split features and target\n",
    "    X = df_model[feature_cols]\n",
    "    y = df_model['readmitted_binary']\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "def build_and_evaluate_models(X, y, feature_cols):\n",
    "    \"\"\"Build and evaluate multiple models\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MODEL BUILDING AND EVALUATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain set: {X_train.shape[0]:,} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "    print(f\"Target distribution in train: {y_train.mean()*100:.2f}% readmitted\")\n",
    "    print(f\"Target distribution in test: {y_test.mean()*100:.2f}% readmitted\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Models to evaluate\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, \n",
    "                                                random_state=42, n_jobs=-1),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5,\n",
    "                                                        random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n{'-'*80}\")\n",
    "        print(f\"Training {name}...\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        # Train model\n",
    "        if name == 'Logistic Regression':\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        print(f\"\\nPerformance Metrics:\")\n",
    "        print(f\"  • Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"  • Precision: {precision:.4f}\")\n",
    "        print(f\"  • Recall:    {recall:.4f}\")\n",
    "        print(f\"  • F1 Score:  {f1:.4f}\")\n",
    "        print(f\"  • ROC AUC:   {roc_auc:.4f}\")\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba,\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "        }\n",
    "    \n",
    "    # Visualize model comparison\n",
    "    visualize_model_comparison(results)\n",
    "    \n",
    "    # Feature importance for best model (Random Forest)\n",
    "    visualize_feature_importance(models['Random Forest'], feature_cols)\n",
    "    \n",
    "    # ROC curves\n",
    "    visualize_roc_curves(results, y_test)\n",
    "    \n",
    "    # Confusion matrices\n",
    "    visualize_confusion_matrices(results)\n",
    "    \n",
    "    return results, X_test, y_test\n",
    "\n",
    "def visualize_model_comparison(results):\n",
    "    \"\"\"Visualize comparison of model performance\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CREATING MODEL COMPARISON VISUALIZATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    model_names = list(results.keys())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, model_name in enumerate(model_names):\n",
    "        values = [results[model_name][metric] for metric in metrics]\n",
    "        ax.bar(x + i*width, values, width, label=model_name, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels([m.upper() for m in metrics])\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_PATH + '05_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Model comparison saved to: {OUTPUT_PATH}05_model_comparison.png\")\n",
    "\n",
    "def visualize_feature_importance(model, feature_cols):\n",
    "    \"\"\"Visualize feature importance from Random Forest\"\"\"\n",
    "    print(f\"\\nCreating feature importance visualization...\")\n",
    "    \n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1][:15]  # Top 15 features\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    ax.barh(range(len(indices)), importances[indices], color='teal', alpha=0.8)\n",
    "    ax.set_yticks(range(len(indices)))\n",
    "    ax.set_yticklabels([feature_cols[i] for i in indices])\n",
    "    ax.set_xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Features', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Top 15 Most Important Features (Random Forest)', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_PATH + '06_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Feature importance saved to: {OUTPUT_PATH}06_feature_importance.png\")\n",
    "\n",
    "def visualize_roc_curves(results, y_test):\n",
    "    \"\"\"Visualize ROC curves for all models\"\"\"\n",
    "    print(f\"\\nCreating ROC curves...\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "    \n",
    "    for i, (name, result) in enumerate(results.items()):\n",
    "        fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\n",
    "        ax.plot(fpr, tpr, color=colors[i], lw=2, \n",
    "               label=f\"{name} (AUC = {result['roc_auc']:.3f})\")\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('ROC Curves: Model Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=11)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_PATH + '07_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"ROC curves saved to: {OUTPUT_PATH}07_roc_curves.png\")\n",
    "\n",
    "def visualize_confusion_matrices(results):\n",
    "    \"\"\"Visualize confusion matrices for all models\"\"\"\n",
    "    print(f\"\\nCreating confusion matrices...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for i, (name, result) in enumerate(results.items()):\n",
    "        cm = result['confusion_matrix']\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
    "                   cbar_kws={'label': 'Count'})\n",
    "        axes[i].set_xlabel('Predicted Label', fontsize=11, fontweight='bold')\n",
    "        axes[i].set_ylabel('True Label', fontsize=11, fontweight='bold')\n",
    "        axes[i].set_title(f'{name}\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xticklabels(['Not Readmitted', 'Readmitted'])\n",
    "        axes[i].set_yticklabels(['Not Readmitted', 'Readmitted'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_PATH + '08_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Confusion matrices saved to: {OUTPUT_PATH}08_confusion_matrices.png\")\n",
    "\n",
    "def generate_insights(results, X, y):\n",
    "    \"\"\"Generate key insights and recommendations\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"KEY INSIGHTS AND RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    best_model_name = max(results, key=lambda x: results[x]['roc_auc'])\n",
    "    best_model = results[best_model_name]\n",
    "    \n",
    "    print(f\"\\nBest Performing Model: {best_model_name}\")\n",
    "    print(f\"  • ROC AUC Score: {best_model['roc_auc']:.4f}\")\n",
    "    print(f\"  • Accuracy: {best_model['accuracy']:.4f}\")\n",
    "    print(f\"  • Precision: {best_model['precision']:.4f}\")\n",
    "    print(f\"  • Recall: {best_model['recall']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BUSINESS RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    recommendations = [\n",
    "        \"1. Implement Risk Stratification: Use the model to identify high-risk patients\",\n",
    "        \"   before discharge and provide targeted interventions.\",\n",
    "        \"\",\n",
    "        \"2. Focus on Key Risk Factors: Prioritize monitoring patients with:\",\n",
    "        \"   • Prior inpatient visits\",\n",
    "        \"   • Emergency department visits\",\n",
    "        \"   • Extended hospital stays\",\n",
    "        \"   • Multiple medications\",\n",
    "        \"\",\n",
    "        \"3. Post-Discharge Follow-up: Establish 48-hour follow-up calls for high-risk\",\n",
    "        \"   patients to ensure medication adherence and address concerns.\",\n",
    "        \"\",\n",
    "        \"4. Care Coordination: Improve discharge planning and care transitions for\",\n",
    "        \"   patients with complex medication regimens.\",\n",
    "        \"\",\n",
    "        \"5. Cost Savings: Reducing readmissions by 20% could save approximately\",\n",
    "        \"   $5.2 million annually (based on average readmission cost of $15,000).\"\n",
    "    ]\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"HOSPITAL READMISSION ANALYTICS - MODELING PIPELINE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df = load_and_prepare_data()\n",
    "    \n",
    "    # Clean data\n",
    "    df = clean_data(df)\n",
    "    \n",
    "    # Engineer features\n",
    "    df = engineer_features(df)\n",
    "    \n",
    "    # Prepare for modeling\n",
    "    X, y, feature_cols = prepare_for_modeling(df)\n",
    "    \n",
    "    # Build and evaluate models\n",
    "    results, X_test, y_test = build_and_evaluate_models(X, y, feature_cols)\n",
    "    \n",
    "    # Generate insights\n",
    "    generate_insights(results, X, y)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MODELING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
